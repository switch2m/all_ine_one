Deamonset vs StatefulSet vs deployment
Deployments: are usually used for stateless applications. However, 
you can save the state of deployment by attaching a Persistent Volume to it
and make it stateful, but all the pods of a deployment will be sharing 
the same Volume and data across all of them will be same.

StatefulSet: is a Kubernetes resource used to manage stateful applications.
It manages the deployment and scaling of a set of Pods, and provides 
guarantee about the ordering and uniqueness of these Pods, unlike 
Deployments it doesn’t create ReplicaSet(so u can't rollback to the previous version)
rather itself creates the Pod 
with a unique naming convention. e.g. If you create a StatefulSet with name
counter, it will create a pod with name counter-0, and for multiple replicas
of a statefulset, their names will increment like counter-0, counter-1, etc
Every replica of a stateful set will have its own state, and each of 
the pods will be creating its own PVC(Persistent Volume Claim).
StatefulSets are useful in case of Databases especially when we need Highly 
Available Databases in production as we create a cluster of Database replicas
with one being the primary replica and others being the secondary replicas. 
The primary will be responsible for read/write operations and secondary for
read only operations and they will be syncing data with the primary one.

DaemonSet: is a controller that ensures that the pod runs on all the nodes
of the cluster. If a node is added/removed from a cluster, DaemonSet 
automatically adds/deletes the pod.
Some typical use cases of a DaemonSet is to run cluster level applications 
like:
    Monitoring Exporters: You would want to monitor all the nodes of your 
    cluster so you will need to run a monitor on all the nodes of the 
    cluster like NodeExporter.
    Logs Collection Daemon: You would want to export logs from all nodes 
    so you would need a DaemonSet of log collector like Fluentd to export
    logs from all your nodes.
---
ASSIGHINT PODS TO NODES
Usecase Scenario:
let's say u want to run an app that runs some data and cpu intensive process
so u want to run those pods belonging to the service on two nodes in the cluster where no other 
pods will run so they will get all the ressources

Why are no pods get scheduled on the master node? is it possible to run a regular pod on the 
master node as well
as we khow pods are automatically scheduled on one the worker nodes and the schedular who
decides intelligently on which worker node the pod would be scheduled based on te worker nodes 
workload utiliaztion(least beasy and have more capacity).
in Some cases we may want to decide ourselves where the pods would be scheduled
and to make this happen there is three way:
- using an attribute callled *nodeName* in the k8s configfile(deployment, pod, stateful...) 
which value would be a specific node name
- if the node names in the cluster are dynamic this is commun in cloud environement where u got 
dynamicaly provisionned cluster node for that we could use a *nodeSelector* attribute and to do
that we should first attach a label to the node where we want the pod to pd scheduled
- if u want to use more flexible expressions to select the proper nodes this is useful when u 
have thousand of nodes in ur cluster runing on different regions, or let’s say we have different 
kinds of workloads running in our cluster and we would like to dedicate, the data processing 
workloads that require higher horsepower to the node that is configured with high or medium 
resources, and here the node affinity comes into play.
in this case we use *nodeAffinity* the affinity language is more expressive and allow u to 
match affinity rules more flexibly with logical operators 
$kubectl get node --show-labels //shows all the node of ur cluster with all their labels
$kubectl label node <node-name> <specify a rundom label should be a key value pair like type=cpu> //labels a node
kubectl label nodes <node-name> <label-key>=<label-value>


---
    example:
    ---
    apiVersion: v1
    kind: Pod
    metadata:
    name: nginx
    labels:
        env: test
    spec:
    containers:
    - name: nginx
        image: nginx
    nodeName: xxx #<specific_node_name>
    #for dynamic cloud environement
    nodeSelector:
        type: cpu
---
---
example: nodeAffinity
apiVersion: v1
kind: Pod
metadata:
 name: dbapp
spec:
 containers:
 - name: dbapp
   image: db-processor
 affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
      - matchExpresions:
        - key: Size
          operator: In #in will tell the scheduler to schedule this pods in the nodes specified on the values
          # there is also notIn which will tell the schedular to not schedule the pods on the specified labeled node on the values attribute
          # and there is also Exit which tell the schedular to check if the the label with key is exeisting and schedule the pod on it, and it dosen't need the values block attribute to be configured
          values: # here we specify the value of the key-value pair of the label
          - Large
          - Medium
          
    #if the pod placement is less important than running the workload itself, in that case, we can set the affinity to the preferred type or in cases where a matching node is not found
    preferredDuringSchedulingIgnoredDuringExecution: #The preferred one is a way of telling the scheduler, hey try your best to place the pod on the matching node defined on the required but if you really cannot find one just place it in here defined in the prefered.
    - weight: 1
      preference:
        matchExpresions:
          - key: xxx #another node key label
            operator: In
            value: 
            - xxx #another node value label
--- 

Now we will walk through the TAINT and TOLERATIONS

TAINT is a property of a node that allows to repel(repousser) a set of pods
unless those pods explicitly tolerate the node taint, in other words if we 
apply a taint to a node it will create a shield around the node which will
prevent the pods to get scheduled inside that node
command to taint o node
$kubectl taint node <Node_Name> <key=value:TAINT_EFFECT>
there are 3 Taint Effects:
NoSchedule: If we apply this taint effect to a node then it will only allow
the pods which have a toleration effect equal to NoSchedule. But if a pod 
is already scheduled in a node and then you apply taint to the node having 
effect NoSchedule, then the pod will remain scheduled in the node.
PreferNoSchedule: in this effect, it will first prefer for no scheduling of
pod but if you have a single node and a PreferNoSchedule taint is applied 
on it. Then even if the pod didn’t tolerate the taint it will get schedule 
inside the node which has a taint effect PreferNoSchedule.
NoExecute: This effect will not only restrict the pod to get scheduled in 
the node but also if a pod is already scheduled in a specific node and we 
have applied a taint of effect NoExecute to the specific node, it will 
immediately throw out the pod outside the node.

TOLERATIONS in pods
As we know that node taints will repel a pod from scheduling in it. So, 
in order to prevent this, Kubernetes provides a concept of pod toleration 
which gives pod an authority to get scheduled on the tainted node, if the 
toleration matches the node taint.
example:
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: noschedule-deployment
spec:
  replicas: 1
  selector:
    matchExpressions:
    - key: name
      operator: In
      values:
      - nginx
  template:
    metadata:
      labels:
        name: nginx
    spec:
      containers:
      - name: nginx-container
        image: nginx
        ports:
        - containerPort: 80
      tolerations: #this will allow this pod to get scheduled on the node taint with size=large and the taint effect of NoSchedule
      - key: size
        operator: "Equal"
        value: large
        effect: NoSchedule
---
If you take a look at the above deployment you will see tolerations block inside podSpec and inside that you will find some keywords like:
1 — key : The value which you have specified while applying node taint.
2 — value : Value that you have mentioned while applying the node taint.
3 — effect : Effect that you have mentioned while applying the node taint.
4 — Operator : There are 2 values of operator Equal and Exists.
Equal: If we specify operator as Equal, then we have to specify all the key
, value, and effect option.
Exists: If we specify operator as Exists then it’s not compulsory to 
mention key, value, and effect option.
If you want to allow your pod to tolerate every node taint then inside the 
pod toleration’s part, you should mention only the operator "Exists" .
By defining this your pod able will tolerate every taint which was applied 
on the node.