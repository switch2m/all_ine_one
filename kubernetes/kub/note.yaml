Deamonset vs StatefulSet vs deployment
Deployments: are usually used for stateless applications. However, 
you can save the state of deployment by attaching a Persistent Volume to it
and make it stateful, but all the pods of a deployment will be sharing 
the same Volume and data across all of them will be same.

StatefulSet: is a Kubernetes resource used to manage stateful applications.
It manages the deployment and scaling of a set of Pods, and provides 
guarantee about the ordering and uniqueness of these Pods, unlike 
Deployments it doesn’t create ReplicaSet(so u can't rollback to the previous version)
rather itself creates the Pod 
with a unique naming convention. e.g. If you create a StatefulSet with name
counter, it will create a pod with name counter-0, and for multiple replicas
of a statefulset, their names will increment like counter-0, counter-1, etc
Every replica of a stateful set will have its own state, and each of 
the pods will be creating its own PVC(Persistent Volume Claim).
StatefulSets are useful in case of Databases especially when we need Highly 
Available Databases in production as we create a cluster of Database replicas
with one being the primary replica and others being the secondary replicas. 
The primary will be responsible for read/write operations and secondary for
read only operations and they will be syncing data with the primary one.

DaemonSet: is a controller that ensures that the pod runs on all the nodes
of the cluster. If a node is added/removed from a cluster, DaemonSet 
automatically adds/deletes the pod.
Some typical use cases of a DaemonSet is to run cluster level applications 
like:
    Monitoring Exporters: You would want to monitor all the nodes of your 
    cluster so you will need to run a monitor on all the nodes of the 
    cluster like NodeExporter.
    Logs Collection Daemon: You would want to export logs from all nodes 
    so you would need a DaemonSet of log collector like Fluentd to export
    logs from all your nodes.
---
ASSIGHINT PODS TO NODES
Usecase Scenario:
let's say u want to run an app that runs some data and cpu intensive process
so u want to run those pods belonging to the service on two nodes in the cluster where no other 
pods will run so they will get all the ressources

Why are no pods get scheduled on the master node? is it possible to run a regular pod on the 
master node as well
as we khow pods are automatically scheduled on one the worker nodes and the schedular who
decides intelligently on which worker node the pod would be scheduled based on te worker nodes 
workload utiliaztion(least beasy and have more capacity).
in Some cases we may want to decide ourselves where the pods would be scheduled
and to make this happen there is three way:
- using an attribute callled *nodeName* in the k8s configfile(deployment, pod, stateful...) 
which value would be a specific node name
- if the node names in the cluster are dynamic this is commun in cloud environement where u got 
dynamicaly provisionned cluster node for that we could use a *nodeSelector* attribute and to do
that we should first attach a label to the node where we want the pod to pd scheduled
- if u want to use more flexible expressions to select the proper nodes this is useful when u 
have thousand of nodes in ur cluster runing on different regions, or let’s say we have different 
kinds of workloads running in our cluster and we would like to dedicate, the data processing 
workloads that require higher horsepower to the node that is configured with high or medium 
resources, and here the node affinity comes into play.
in this case we use *nodeAffinity* the affinity language is more expressive and allow u to 
match affinity rules more flexibly with logical operators 
$kubectl get node --show-labels //shows all the node of ur cluster with all their labels
$kubectl label node <node-name> <specify a rundom label should be a key value pair like type=cpu> //labels a node
kubectl label nodes <node-name> <label-key>=<label-value>


---
    example:
    ---
    apiVersion: v1
    kind: Pod
    metadata:
    name: nginx
    labels:
        env: test
    spec:
    containers:
    - name: nginx
        image: nginx
    nodeName: xxx #<specific_node_name>
    #for dynamic cloud environement
    nodeSelector:
        type: cpu
---
---
example: nodeAffinity
apiVersion: v1
kind: Pod
metadata:
 name: dbapp
spec:
 containers:
 - name: dbapp
   image: db-processor
 affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
      - matchExpresions:
        - key: Size
          operator: In #in will tell the scheduler to schedule this pods in the nodes specified on the values
          # there is also notIn which will tell the schedular to not schedule the pods on the specified labeled node on the values attribute
          # and there is also Exit which tell the schedular to check if the the label with key is exeisting and schedule the pod on it, and it dosen't need the values block attribute to be configured
          values: # here we specify the value of the key-value pair of the label
          - Large
          - Medium
          
    #if the pod placement is less important than running the workload itself, in that case, we can set the affinity to the preferred type or in cases where a matching node is not found
    preferredDuringSchedulingIgnoredDuringExecution: #The preferred one is a way of telling the scheduler, hey try your best to place the pod on the matching node defined on the required but if you really cannot find one just place it in here defined in the prefered.
    - weight: 1
      preference:
        matchExpresions:
          - key: xxx #another node key label
            operator: In
            value: 
            - xxx #another node value label
